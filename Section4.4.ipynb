{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430c7766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASS DEFINITIONS FOR NEURAL NETWORKS USED IN DEEP GALERKIN METHOD\n",
    "\n",
    "#%% import needed packages\n",
    "import tensorflow as tf\n",
    "class DenseLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, output_dim, input_dim, transformation=None):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dim:       dimensionality of input data\n",
    "            output_dim:      number of outputs for dense layer\n",
    "            transformation:  activation function used inside the layer; using\n",
    "                             None is equivalent to the identity map \n",
    "        \n",
    "        Returns: customized Keras (fully connected) layer object \n",
    "        '''        \n",
    "        \n",
    "        # create an instance of a Layer object (call initialize function of superclass of DenseLayer)\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.W = self.add_weight(\n",
    "            name=\"W\",\n",
    "            shape=[self.input_dim, self.output_dim],\n",
    "            initializer=tf.initializers.GlorotUniform(),\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        # Create the bias vector\n",
    "        self.b = self.add_weight(\n",
    "            name=\"b\",\n",
    "            shape=[1, self.output_dim],\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            dtype=tf.float32)\n",
    "        \n",
    "        if transformation:\n",
    "            if transformation == \"tanh\":\n",
    "                self.transformation = tf.tanh\n",
    "            elif transformation == \"relu\":\n",
    "                self.transformation = tf.nn.relu\n",
    "            elif transformation == \"mish\":\n",
    "                self.transformation = lambda x: x * tf.math.tanh(tf.math.softplus(x))\n",
    "            elif transformation == \"swish\":\n",
    "                self.transformation = lambda x: x * tf.math.sigmoid(x)\n",
    "            else:\n",
    "                self.transformation = None\n",
    "        else:\n",
    "            self.transformation = None\n",
    "\n",
    "    def call(self, X):\n",
    "        '''Compute output of a dense layer for a given input X \n",
    "        \n",
    "        Args:                        \n",
    "            X: input to layer            \n",
    "        '''\n",
    "        \n",
    "        # compute dense layer output\n",
    "        S = tf.add(tf.matmul(X, self.W), self.b)\n",
    "                \n",
    "        if self.transformation:\n",
    "            S = self.transformation(S)\n",
    "        \n",
    "        return S\n",
    "\n",
    "def terminal_utility(xy):\n",
    "    x = xy[:, 0]\n",
    "    y = xy[:, 1]\n",
    "    result = x - tf.exp(y)\n",
    "    return tf.expand_dims(result, axis=-1)\n",
    "  \n",
    "class DGMNet(tf.keras.Model):\n",
    "    def __init__(self, layer_width, n_layers, input_dim, final_trans=None, feedforward=False, output_dim=1, control_output=False):\n",
    "        super(DGMNet, self).__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.feedforward = feedforward\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Define kernel layers\n",
    "        self.kernel_layers = []\n",
    "        for _ in range(n_layers):\n",
    "            self.kernel_layers.append(DenseLayer(layer_width, layer_width, transformation='swish'))\n",
    "\n",
    "        # Final output layer\n",
    "        self.output_weight = tf.keras.layers.Dense(output_dim, activation=final_trans)\n",
    "        self.control_output = control_output\n",
    "        self.initial_layer = DenseLayer(layer_width, input_dim+1, transformation='swish')\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, t, x):\n",
    "        # Concatenate time and space\n",
    "        X = tf.concat([t, x], axis=1)\n",
    "        S = self.initial_layer.call(X)\n",
    "        for i in range(self.n_layers):\n",
    "            S = self.kernel_layers[i](S)+S\n",
    "\n",
    "        # Final output\n",
    "        result = self.output_weight(S)\n",
    "        if self.control_output == False:\n",
    "            result = terminal_utility(x)+result*(1-t)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd7cd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T18:01:26.518366Z",
     "iopub.status.busy": "2025-06-12T18:01:26.517338Z",
     "iopub.status.idle": "2025-06-13T00:44:04.317165Z",
     "shell.execute_reply": "2025-06-13T00:44:04.315195Z"
    },
    "papermill": {
     "duration": 24157.81622,
     "end_time": "2025-06-13T00:44:04.319779",
     "exception": false,
     "start_time": "2025-06-12T18:01:26.503559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%% import needed packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "#%% Parameters \n",
    "d = 2 # dimension\n",
    "sigma = 1  # asset volatility\n",
    "T = 1        # terminal time (investment horizon)\n",
    "\n",
    "# Solution parameters (domain on which to solve PDE)\n",
    "t_low = 0.0    # time lower bound\n",
    "X_low = 0.0    # wealth lower bound\n",
    "X_high = 1.0           # wealth upper bound\n",
    "\n",
    "# neural network parameters\n",
    "num_layers = 3\n",
    "nodes_per_layer = 32\n",
    "starting_learning_rate = 0.001\n",
    "\n",
    "# Training parameters\n",
    "sampling_stages  = 50000  # number of times to resample new time-space domain points\n",
    "steps_per_sample = 10    # number of SGD steps to take before re-sampling\n",
    "\n",
    "# Sampling parameters\n",
    "nSim_interior = 2000\n",
    "nSim_interior_val = 2000\n",
    "nSim_terminal = 1\n",
    "\n",
    "# multipliers for oversampling i.e. draw X from [X_low - X_oversample, X_high + X_oversample]\n",
    "X_oversample = 0.5\n",
    "t_oversample = 0.0\n",
    "\n",
    "# Plot options\n",
    "n_plot = 41  # Points on plot grid for each dimension\n",
    "\n",
    "\n",
    "#%% Analytical Solution\n",
    "def h(t, xy):\n",
    "    x = xy[:, 0][:, np.newaxis]  \n",
    "    y = xy[:, 1][:, np.newaxis]  \n",
    "    term1 = x - np.exp(y)\n",
    "    term2 = 0.25 * np.exp(-y) * x * x\n",
    "    term3 = np.exp(sigma**2 * (T - t)) - 1\n",
    "    return term1 + term2 * term3\n",
    "\n",
    "def terminal_utility(xy):\n",
    "    x = xy[:, 0]\n",
    "    y = xy[:, 1]\n",
    "    result = x - tf.exp(y)\n",
    "    return tf.expand_dims(result, axis=-1)  \n",
    "\n",
    "def sampler(nSim_interior, nSim_terminal):\n",
    "    ''' Sample time-space points from the function's domain; points are sampled\n",
    "        uniformly on the interior of the domain, at the initial/terminal time points\n",
    "        and along the spatial boundary at different time points. \n",
    "    \n",
    "    Args:\n",
    "        nSim_interior: number of space points in the interior of the function's domain to sample \n",
    "        nSim_terminal: number of space points at terminal time to sample (terminal condition)\n",
    "    '''\n",
    "    # Sampler #1: domain interior    \n",
    "    t_interior = np.random.uniform(low=t_low - t_oversample*(T-t_low), high=T, size=[nSim_interior, 1]).astype(np.float32)\n",
    "    X_interior = np.random.uniform(low=X_low - X_oversample*(X_high-X_low), high=X_high + X_oversample*(X_high-X_low), size=[nSim_interior, d]).astype(np.float32)\n",
    "    \n",
    "\n",
    "    # Sampler #3: initial/terminal condition\n",
    "    t_terminal = T * np.ones((nSim_terminal, 1)).astype(np.float32)\n",
    "    X_terminal = np.random.uniform(low=X_low - X_oversample*(X_high-X_low), high=X_high + X_oversample*(X_high-X_low), size = [nSim_terminal, d]).astype(np.float32)\n",
    "\n",
    "\n",
    "    return t_interior, X_interior, t_terminal, X_terminal\n",
    "\n",
    "\n",
    "\n",
    "#%% Loss function for Merton Problem PDE\n",
    "\n",
    "def loss(model,control, t_interior, X_interior, t_terminal, X_terminal):\n",
    "    ''' Compute total loss for training.'''\n",
    "\n",
    "    with tf.GradientTape(persistent=True, watch_accessed_variables=False) as gt:\n",
    "        gt.watch(t_interior)\n",
    "        gt.watch(X_interior)\n",
    "        V = model(t_interior, X_interior)  # V shape: (?, 1)\n",
    "        V_1 = gt.gradient(V, X_interior)   # First-order gradients, shape: (?, 2)\n",
    "        \n",
    "    V_t = gt.gradient(V, t_interior)  # Time derivative, shape: (?, 1)\n",
    "    Z = control(t_interior, X_interior)  # Control function, shape: (?, 1)\n",
    "    V_1_x = tf.expand_dims(V_1[:, 0], axis=-1)  # Shape: [4, 1]\n",
    "    V_1_y = tf.expand_dims(V_1[:, 1], axis=-1)\n",
    "    V_2 = gt.batch_jacobian(V_1, X_interior)  # shape (?, 2, 2)\n",
    "    V_xx = tf.expand_dims(V_2[:, 0, 0], axis=-1)  # d²V/dx²\n",
    "    V_yy = tf.expand_dims(V_2[:, 1, 1], axis=-1)  # d²V/dy²\n",
    "    V_xy = tf.expand_dims(V_2[:, 0, 1], axis=-1)  # d²V/dxdy\n",
    "\n",
    "    # Extract x and y\n",
    "    x_x = tf.expand_dims(X_interior[:, 0], axis=-1)  #  (?, 1)\n",
    "    x_y = tf.expand_dims(X_interior[:, 1], axis=-1)  #  (?, 1)\n",
    "\n",
    "    # Compute terms\n",
    "    term1 = sigma * sigma * x_x * x_x * Z * V_1_x  # shape (?, 1)\n",
    "    term2 = 0.5 * sigma * sigma * x_x * x_x * Z * Z * V_1_y  # shape (?, 1)\n",
    "    term3 = 0.5 * sigma * sigma * x_x * x_x * (V_xx + Z * Z * V_yy)  # shape (?, 1)\n",
    "    term4 = sigma * sigma * x_x * x_x * Z * V_xy  # shape (?, 1)\n",
    "\n",
    "    # Compute the PDE residual\n",
    "    diff_V = V_t + term1 + term2 + term3 + term4  # shape (?, 1)\n",
    "\n",
    "    L1 = tf.reduce_mean(tf.square(diff_V))  # Loss term for PDE\n",
    "\n",
    "\n",
    "    target_terminal = terminal_utility(X_terminal)\n",
    "    fitted_terminal = model(t_terminal, X_terminal)\n",
    "    diff_terminal = fitted_terminal - target_terminal\n",
    "    L3 = tf.reduce_mean(tf.square(fitted_terminal - target_terminal) )  # Loss term for terminal condition\n",
    "    \n",
    "    del gt\n",
    "    return L1, L3, diff_V, diff_terminal\n",
    "\n",
    "def loss_control(model, control, t_interior, X_interior, t_terminal, X_terminal):\n",
    "    ''' Compute total loss for training.'''\n",
    "    with tf.GradientTape(persistent=True, watch_accessed_variables=False) as gt:\n",
    "        gt.watch(t_interior)\n",
    "        gt.watch(X_interior)\n",
    "        V = model(t_interior, X_interior)  # V shape: (?, 1)\n",
    "        V_1 = gt.gradient(V, X_interior)   # First-order gradients, shape: (?, 2)\n",
    "        \n",
    "    V_t = gt.gradient(V, t_interior)  # Time derivative, shape: (?, 1)\n",
    "    Z = control(t_interior, X_interior)  # Control function, shape: (?, 1)\n",
    "    V_1_x = tf.expand_dims(V_1[:, 0], axis=-1)  # Shape: [4, 1]\n",
    "    V_1_y = tf.expand_dims(V_1[:, 1], axis=-1)\n",
    "    # Compute second-order derivatives\n",
    "    V_2 = gt.batch_jacobian(V_1, X_interior)  # shape (?, 2, 2)\n",
    "\n",
    "    V_xx = tf.expand_dims(V_2[:, 0, 0], axis=-1)  # d²V/dx²\n",
    "    V_yy = tf.expand_dims(V_2[:, 1, 1], axis=-1)  # d²V/dy²\n",
    "    V_xy = tf.expand_dims(V_2[:, 0, 1], axis=-1)  # d²V/dxdy\n",
    "\n",
    "    # Extract x and y\n",
    "    x_x = tf.expand_dims(X_interior[:, 0], axis=-1)  # (?, 1)\n",
    "    x_y = tf.expand_dims(X_interior[:, 1], axis=-1)  # (?, 1)\n",
    "\n",
    "    # Compute terms\n",
    "    term1 = sigma * sigma * x_x * x_x * Z * V_1_x  # shape (?, 1)\n",
    "    term2 = 0.5 * sigma * sigma * x_x * x_x * Z * Z * V_1_y # shape (?, 1)\n",
    "    term3 = 0.5 * sigma * sigma * x_x * x_x * (V_xx + Z * Z * V_yy)  # shape (?, 1)\n",
    "    term4 = sigma * sigma * x_x * x_x * Z * V_xy  # shape (?, 1)\n",
    "\n",
    "    # Compute the PDE residual\n",
    "    f = term1 + term2 + term3 + term4  # shape (?, 1)\n",
    "\n",
    "    L2 = tf.reduce_mean(f)  # Loss term for control\n",
    "    dfdz = sigma * sigma * x_x * x_x * V_1_x + \\\n",
    "           sigma * sigma * x_x * x_x * Z * V_1_y + \\\n",
    "            sigma * sigma * x_x * x_x * Z * V_yy + \\\n",
    "            sigma * sigma * x_x * x_x * V_xy\n",
    "    dfdz = tf.reduce_mean(dfdz) # derivative of f(L2) on control\n",
    "    del gt\n",
    "    return -L2, dfdz\n",
    "\n",
    "#%% Set up network\n",
    "\n",
    "# initialize DGM model\n",
    "model = DGMNet(nodes_per_layer, num_layers, input_dim=d, output_dim=1)\n",
    "control = DGMNet(nodes_per_layer, num_layers, input_dim=d,output_dim=1, control_output=True)\n",
    "\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=0.001, decay_steps=100000, end_learning_rate=0.00001, power=0.8\n",
    ")\n",
    "\n",
    "# Optimizers\n",
    "optimizer_value = tf.keras.optimizers.Adam(lr_schedule)\n",
    "optimizer_control = tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "\n",
    "#%% Train network\n",
    "\n",
    "# Define the train_step function\n",
    "@tf.function\n",
    "def train_step(model, control, t_interior, X_interior, t_terminal, X_terminal, optimizer_value, optimizer_control):\n",
    "    ''' Perform of single training step.'''\n",
    "    # Compute loss for the value function (L1 + L3)\n",
    "    t_interior = tf.convert_to_tensor(t_interior, dtype=tf.float32)\n",
    "    X_interior = tf.convert_to_tensor(X_interior, dtype=tf.float32)\n",
    "    t_terminal = tf.convert_to_tensor(t_terminal, dtype=tf.float32)\n",
    "    X_terminal = tf.convert_to_tensor(X_terminal, dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        L1, L3, diff_V, diff_terminal = loss(model, control, t_interior, X_interior, t_terminal, X_terminal)\n",
    "        total_loss = L1+L3\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "\n",
    "    optimizer_value.apply_gradients(zip(grads, model.trainable_variables))  # Update the value model\n",
    "    \n",
    "    # Compute loss for the control function (L2)\n",
    "    with tf.GradientTape() as tape1:\n",
    "        L2,_ = loss_control(model, control, t_interior, X_interior, t_terminal, X_terminal)\n",
    "    grads_control = tape1.gradient(L2, control.trainable_variables)\n",
    "    optimizer_control.apply_gradients(zip(grads_control, control.trainable_variables))  # Update the control model\n",
    "    \n",
    "    return L1, L3, diff_V, diff_terminal, L2, total_loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def eval_model(model, control, t_interior_val, X_interior_val, t_terminal_val, X_terminal_val):\n",
    "    L1_val, L3_val, diff_V_val, diff_terminal_val = loss(model, control, t_interior_val, X_interior_val, t_terminal_val, X_terminal_val)\n",
    "    L2_val, dL2dZ_val = loss_control(model, control, t_interior_val, X_interior_val, t_terminal_val, X_terminal_val)\n",
    "    return L1_val, L3_val, diff_V_val, diff_terminal_val, L2_val, dL2dZ_val\n",
    "\n",
    "# Initialize lists for storing loss values\n",
    "loss_list = []\n",
    "L2_list = []\n",
    "L1_list = []\n",
    "L3_list = []\n",
    "value_list = []\n",
    "control_list = []\n",
    "diffV_list = []\n",
    "diffZ_list = []\n",
    "\n",
    "maxdiff_V = 100\n",
    "maxdiff_terminal = 100\n",
    "\n",
    "# dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "t_eval = tf.constant([[0.0]], dtype=tf.float32)  # shape (1, 1)\n",
    "X_eval = tf.constant([[1.0, 0.0]], dtype=tf.float32)  # shape (1, 3)\n",
    "\n",
    "time_record = []\n",
    "start_time = time.time()\n",
    "# Main training loop\n",
    "for i in range(sampling_stages):\n",
    "    t_interior, X_interior, t_terminal, X_terminal = sampler(nSim_interior, nSim_terminal)\n",
    "    for _ in range(steps_per_sample):\n",
    "        L1, L3, diff_V, diff_terminal, L2, total_loss = train_step(\n",
    "                model, control, t_interior, X_interior,\n",
    "                t_terminal, X_terminal,\n",
    "                optimizer_value, optimizer_control\n",
    "            )\n",
    "    loss_list.append(total_loss)\n",
    "    L2_list.append(L2)\n",
    "    L1_list.append(L1)\n",
    "    L3_list.append(L3)\n",
    "\n",
    "    # val\n",
    "    t_interior_val, X_interior_val, t_terminal_val, X_terminal_val = sampler(nSim_interior_val, nSim_terminal)\n",
    "    t_interior_val = tf.convert_to_tensor(t_interior_val, dtype=tf.float32)\n",
    "    X_interior_val = tf.convert_to_tensor(X_interior_val, dtype=tf.float32)\n",
    "    t_terminal_val = tf.convert_to_tensor(t_terminal_val, dtype=tf.float32)\n",
    "    X_terminal_val = tf.convert_to_tensor(X_terminal_val, dtype=tf.float32)\n",
    "\n",
    "    L1_val, L3_val, diff_V_val, diff_terminal_val, L2_val, dL2dZ_val = eval_model(model, control, t_interior_val, X_interior_val, t_terminal_val, X_terminal_val)\n",
    "\n",
    "    maxdiff_V = np.max(np.abs(diff_V_val.numpy()))\n",
    "    maxdiff_terminal = np.max(np.abs(diff_terminal_val.numpy()))\n",
    "    maxdL2dZ = np.max(np.abs(dL2dZ_val.numpy()))\n",
    "    diffV_list.append(maxdiff_V)\n",
    "    diffZ_list.append(maxdL2dZ)\n",
    "\n",
    "    print(f\"Stage {i}:{maxdiff_V},{maxdiff_terminal},{maxdL2dZ},L1 = {L1.numpy()}, L3 = {L3.numpy()}, L2 = {L2.numpy()}\")\n",
    "\n",
    "    # Evaluate at (t=0, x=1, y=0)\n",
    "    V_pred = model(t_eval, X_eval)\n",
    "    value_list.append(V_pred.numpy()[0, 0])\n",
    "    print(\"V(0, 1, 0) =\", V_pred.numpy()[0, 0])\n",
    "    control_pred = control(t_eval, X_eval)\n",
    "    control_list.append(control_pred.numpy()[0, 0])\n",
    "    print(\"z(0, 1, 0) =\", control_pred.numpy()[0, 0])\n",
    "    end_time = time.time()\n",
    "    time_record.append(end_time - start_time)\n",
    "    if maxdiff_V < 1e-3 and maxdL2dZ < 1e-3:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25de9e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T00:44:11.946088Z",
     "iopub.status.busy": "2025-06-13T00:44:11.945264Z",
     "iopub.status.idle": "2025-06-13T00:44:14.914957Z",
     "shell.execute_reply": "2025-06-13T00:44:14.914118Z"
    },
    "papermill": {
     "duration": 4.858281,
     "end_time": "2025-06-13T00:44:14.916903",
     "exception": false,
     "start_time": "2025-06-13T00:44:10.058622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def to_float_list(tensor_list):\n",
    "    return [float(t.numpy()) if isinstance(t, tf.Tensor) else t for t in tensor_list]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'diffV': to_float_list(diffV_list),\n",
    "    'diffZ': to_float_list(diffZ_list),\n",
    "    'loss': to_float_list(loss_list),\n",
    "    'L1': to_float_list(L1_list),\n",
    "    'L3': to_float_list(L3_list),\n",
    "    'L2': to_float_list(L2_list),\n",
    "    'V_pred': value_list,\n",
    "    'control_pred': control_list,\n",
    "})\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "df.to_csv(f'result_{now}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f408f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T00:44:38.630332Z",
     "iopub.status.busy": "2025-06-13T00:44:38.629593Z",
     "iopub.status.idle": "2025-06-13T00:44:39.339676Z",
     "shell.execute_reply": "2025-06-13T00:44:39.338699Z"
    },
    "papermill": {
     "duration": 2.607367,
     "end_time": "2025-06-13T00:44:39.342267",
     "exception": false,
     "start_time": "2025-06-13T00:44:36.734900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x1 = np.linspace(X_low, X_high, n_plot**2)\n",
    "x2 = np.linspace(X_low, X_high, n_plot**2)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "# Create mesh for t=1 and the x coordinates (flattened)\n",
    "t_mesh = np.ones((n_plot**2* n_plot**2, 1), dtype=np.float32)  # t=1\n",
    "x_mesh = np.stack([X1.flatten(), X2.flatten()], axis=1).astype(np.float32)\n",
    "\n",
    "# Evaluate the model output (Numerical solution)\n",
    "h_numerical = model(t_mesh, x_mesh)\n",
    "# Evaluate the analytical solution\n",
    "h_analytical = h(1, x_mesh)\n",
    "\n",
    "# Convert to NumPy arrays for further handling\n",
    "h_numerical = h_numerical.numpy().reshape(n_plot**2, n_plot**2)\n",
    "h_analytical = h_analytical.reshape(n_plot**2, n_plot**2)\n",
    "print(h_numerical,h_analytical)\n",
    "\n",
    "# Compute the absolute error between numerical and analytical solutions\n",
    "error = np.abs(h_numerical - h_analytical)\n",
    "\n",
    "# Plot the error as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pcolormesh(X1, X2, error, cmap='rainbow', shading='auto')\n",
    "plt.colorbar(label='Error: h_numerical - h_analytical')\n",
    "plt.xlabel('x1', fontsize=14)\n",
    "plt.ylabel('x2', fontsize=14)\n",
    "plt.title('Absolute Error of H(1,x)', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23ff883",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T00:44:43.134620Z",
     "iopub.status.busy": "2025-06-13T00:44:43.133841Z",
     "iopub.status.idle": "2025-06-13T00:44:43.684173Z",
     "shell.execute_reply": "2025-06-13T00:44:43.683365Z"
    },
    "papermill": {
     "duration": 2.442075,
     "end_time": "2025-06-13T00:44:43.686395",
     "exception": false,
     "start_time": "2025-06-13T00:44:41.244320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x1 = np.linspace(X_low, X_high, n_plot**2)\n",
    "x2 = np.linspace(X_low, X_high, n_plot**2)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "# Create mesh for t=1 and the x coordinates (flattened)\n",
    "t_mesh = np.zeros((n_plot**2*n_plot**2, 1), dtype=np.float32)  # t=1\n",
    "x_mesh = np.stack([X1.flatten(), X2.flatten()], axis=1).astype(np.float32)\n",
    "\n",
    "# Evaluate the model output (Numerical solution)\n",
    "h_numerical = model(t_mesh, x_mesh)\n",
    "\n",
    "\n",
    "h_analytical = h(0, x_mesh)\n",
    "\n",
    "# Convert to NumPy arrays for further handling\n",
    "h_numerical = h_numerical.numpy().reshape(n_plot**2, n_plot**2)\n",
    "h_analytical = h_analytical.reshape(n_plot**2, n_plot**2)\n",
    "print(h_numerical,h_analytical)\n",
    "\n",
    "# Compute the absolute error between numerical and analytical solutions\n",
    "error = np.abs(h_numerical - h_analytical)\n",
    "\n",
    "# Plot the error as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pcolormesh(X1, X2, error, cmap='rainbow', shading='auto')\n",
    "plt.colorbar(label='Error: h_numerical - h_analytical')\n",
    "plt.xlabel('x1', fontsize=14)\n",
    "plt.ylabel('x2', fontsize=14)\n",
    "plt.title('Absolute Error of H(0,x)', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c(t, xy):\n",
    "    x = xy[:, 0][:, np.newaxis]  \n",
    "    y = xy[:, 1][:, np.newaxis]\n",
    "    return 1.0 / (2.0 * np.exp(y))\n",
    "x1 = np.linspace(X_low, X_high, n_plot**2)\n",
    "x2 = np.linspace(X_low, X_high, n_plot**2)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "# Create mesh for t=1 and the x coordinates (flattened)\n",
    "t_mesh = np.zeros((n_plot**2*n_plot**2, 1), dtype=np.float32)  # t=1\n",
    "x_mesh = np.stack([X1.flatten(), X2.flatten()], axis=1).astype(np.float32)\n",
    "\n",
    "# Evaluate the model output (Numerical solution)\n",
    "z_numerical = control(t_mesh, x_mesh)\n",
    "z_analytical = c(0, x_mesh)\n",
    "\n",
    "# Convert to NumPy arrays for further handling\n",
    "z_numerical = z_numerical.numpy().reshape(n_plot**2, n_plot**2)\n",
    "z_analytical = z_analytical.reshape(n_plot**2, n_plot**2)\n",
    "print(z_numerical,z_analytical)\n",
    "\n",
    "# Compute the absolute error between numerical and analytical solutions\n",
    "error = np.abs(z_numerical - z_analytical)\n",
    "\n",
    "# Plot the error as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pcolormesh(X1, X2, error, cmap='rainbow', shading='auto')\n",
    "plt.colorbar(label='Error: z_numerical - z_analytical')\n",
    "plt.xlabel('x1', fontsize=14)\n",
    "plt.ylabel('x2', fontsize=14)\n",
    "plt.title('Absolute Error of Z(0,x)', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b293cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# save model weights\n",
    "model.save_weights(f\"model_{now}.weights.h5\")\n",
    "control.save_weights(f\"modelcontrol_{now}.weights.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 24239.341074,
   "end_time": "2025-06-13T00:45:24.526530",
   "environment_variables": {},
   "exception": null,
   "input_path": "x5.ipynb",
   "output_path": "x51.ipynb",
   "parameters": {},
   "start_time": "2025-06-12T18:01:25.185456",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
