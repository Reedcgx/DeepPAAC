{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f947d57-a5b5-4ee8-8777-3d167174b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASS DEFINITIONS FOR NEURAL NETWORKS USED IN DEEP GALERKIN METHOD\n",
    "\n",
    "#%% import needed packages\n",
    "import tensorflow as tf\n",
    "\n",
    "class DenseLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, output_dim, input_dim, transformation=None):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dim:       dimensionality of input data\n",
    "            output_dim:      number of outputs for dense layer\n",
    "            transformation:  activation function used inside the layer; using\n",
    "                             None is equivalent to the identity map \n",
    "        \n",
    "        Returns: customized Keras (fully connected) layer object \n",
    "        '''        \n",
    "        \n",
    "        # create an instance of a Layer object (call initialize function of superclass of DenseLayer)\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.W = self.add_weight(\n",
    "            name=\"W\",\n",
    "            shape=[self.input_dim, self.output_dim],\n",
    "            initializer=tf.initializers.GlorotUniform(),\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        # Create the bias vector\n",
    "        self.b = self.add_weight(\n",
    "            name=\"b\",\n",
    "            shape=[1, self.output_dim],\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            dtype=tf.float32)\n",
    "        \n",
    "        if transformation:\n",
    "            if transformation == \"tanh\":\n",
    "                self.transformation = tf.tanh\n",
    "            elif transformation == \"relu\":\n",
    "                self.transformation = tf.nn.relu\n",
    "            elif transformation == \"mish\":\n",
    "                self.transformation = lambda x: x * tf.math.tanh(tf.math.softplus(x))\n",
    "            elif transformation == \"swish\":\n",
    "                self.transformation = lambda x: x * tf.math.sigmoid(x)\n",
    "            else:\n",
    "                self.transformation = None\n",
    "        else:\n",
    "            self.transformation = None\n",
    "\n",
    "    def call(self, X):\n",
    "        '''Compute output of a dense layer for a given input X \n",
    "        \n",
    "        Args:                        \n",
    "            X: input to layer            \n",
    "        '''\n",
    "        \n",
    "        # compute dense layer output\n",
    "        S = tf.add(tf.matmul(X, self.W), self.b)\n",
    "                \n",
    "        if self.transformation:\n",
    "            S = self.transformation(S)\n",
    "        \n",
    "        return S\n",
    "\n",
    "def terminal_utility(x):\n",
    "    return -x\n",
    "  \n",
    "class DGMNet(tf.keras.Model):\n",
    "    def __init__(self, layer_width, n_layers, input_dim, final_trans=None, feedforward=False, output_dim=1, control_output=False):\n",
    "        super(DGMNet, self).__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.feedforward = feedforward\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Define kernel layers\n",
    "        self.kernel_layers = []\n",
    "        for _ in range(n_layers):\n",
    "            self.kernel_layers.append(DenseLayer(layer_width, layer_width, transformation='swish'))\n",
    "\n",
    "        # Final output layer\n",
    "        self.output_weight = tf.keras.layers.Dense(output_dim, activation=final_trans)\n",
    "        self.control_output = control_output\n",
    "        self.initial_layer = DenseLayer(layer_width, input_dim+1, transformation='swish')\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, t, x):\n",
    "        # Concatenate time and space\n",
    "        X = tf.concat([t, x], axis=1)\n",
    "        S = self.initial_layer.call(X)\n",
    "        for i in range(self.n_layers):\n",
    "            S = self.kernel_layers[i](S)+S\n",
    "\n",
    "        # Final output\n",
    "        result = self.output_weight(S)\n",
    "        if self.control_output == False:\n",
    "            result = terminal_utility(x)+result*(1-t)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703325ff-5e20-4d10-a05f-fd9b099edc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% import needed packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#%% Parameters \n",
    "d = 1 # dimension\n",
    "sigma = 1  # asset volatility\n",
    "T = 1        # terminal time (investment horizon)\n",
    "#Cbeta_low=0  # Lower bound for Beta case 1\n",
    "#Cbeta_up=0.1   # Upper bound for Beta case 1\n",
    "#CbetaZ_low=0    # Lower bound for Beta + Z case 1\n",
    "#CbetaZ_up=0.5    # Upper bound for Beta + Z case 1\n",
    "\n",
    "#Cbeta_low=0  # Lower bound for Beta case 2\n",
    "#Cbeta_up=0.2   # Upper bound for Beta case 2\n",
    "\n",
    "Cbeta_up=0.5   # Upper bound for Beta case 3\n",
    "CbetaZ_up=1.2   # Upper bound for Beta + Z case 3\n",
    "\n",
    "\n",
    "# Solution parameters (domain on which to solve PDE)\n",
    "t_low = 0 - 1e-10    # time lower bound\n",
    "X_low = 0- 1e-10  # wealth lower bound\n",
    "X_high = 1.0           # wealth upper bound\n",
    "\n",
    "# neural network parameters\n",
    "num_layers = 3\n",
    "nodes_per_layer = 64\n",
    "starting_learning_rate = 0.001\n",
    "\n",
    "# Training parameters\n",
    "sampling_stages  = 5000  # number of times to resample new time-space domain points\n",
    "steps_per_sample = 10    # number of SGD steps to take before re-sampling\n",
    "\n",
    "# Sampling parameters\n",
    "nSim_interior = 4000\n",
    "nSim_terminal = 1\n",
    "\n",
    "# multipliers for oversampling i.e. draw X from [X_low - X_oversample, X_high + X_oversample]\n",
    "X_oversample = 0.5\n",
    "t_oversample = 0.0\n",
    "\n",
    "# Plot options\n",
    "n_plot = 41  # Points on plot grid for each dimension\n",
    "\n",
    "# Save options\n",
    "saveOutput = False\n",
    "saveName   = 'Problem'\n",
    "saveFigure = False\n",
    "figureName = 'Problem'\n",
    "\n",
    "\n",
    "#%% Analytical Solution\n",
    "def h(t, x):\n",
    "    # return 3*(T-t)/8-x #case 1\n",
    "    #return (T-t)/2-x #case 2\n",
    "    return (T-t)/2-x #case 3\n",
    "\n",
    "def sampler(nSim_interior, nSim_terminal):\n",
    "    ''' Sample time-space points from the function's domain; points are sampled\n",
    "        uniformly on the interior of the domain, at the initial/terminal time points\n",
    "        and along the spatial boundary at different time points. \n",
    "    \n",
    "    Args:\n",
    "        nSim_interior: number of space points in the interior of the function's domain to sample \n",
    "        nSim_terminal: number of space points at terminal time to sample (terminal condition)\n",
    "    '''\n",
    "    # Sampler #1: domain interior    \n",
    "    t_interior = np.random.uniform(low=t_low - t_oversample*(T-t_low), high=T, size=[nSim_interior, 1]).astype(np.float32)\n",
    "    X_interior = np.random.uniform(low=X_low - X_oversample*(X_high-X_low), high=X_high + X_oversample*(X_high-X_low), size=[nSim_interior, d]).astype(np.float32)\n",
    "    \n",
    "\n",
    "    # Sampler #3: initial/terminal condition\n",
    "    t_terminal = T * np.ones((nSim_terminal, 1)).astype(np.float32)\n",
    "    X_terminal = np.random.uniform(low=X_low - X_oversample*(X_high-X_low), high=X_high + X_oversample*(X_high-X_low), size = [nSim_terminal, d]).astype(np.float32)\n",
    "\n",
    "\n",
    "    return t_interior, X_interior, t_terminal, X_terminal\n",
    "\n",
    "#%% Loss function for Merton Problem PDE\n",
    "\n",
    "def loss(model,control, t_interior, X_interior, t_terminal, X_terminal):\n",
    "    ''' Compute total loss for training.'''\n",
    "\n",
    "    with tf.GradientTape(persistent=True, watch_accessed_variables=False) as gt:\n",
    "        gt.watch(t_interior)\n",
    "        gt.watch(X_interior)\n",
    "        V = model(t_interior, X_interior)  # V shape: (?, 1)\n",
    "        V_x = gt.gradient(V, X_interior)   # First-order gradients, shape: (?, 1)\n",
    "        \n",
    "    V_t = gt.gradient(V, t_interior)  # Time derivative, shape: (?, 1)\n",
    "    out = control(t_interior, X_interior)\n",
    "    Z = tf.expand_dims(out[:,0],axis=-1)  # Control function, shape: (?, 1)\n",
    "    alpha=tf.expand_dims(out[:,1],axis=-1) # Control function, shape: (?, 1)\n",
    "    beta=tf.expand_dims(out[:,2],axis=-1) # Control function, shape: (?, 1)\n",
    "    V_xx = gt.batch_jacobian(V_x, X_interior)  # shape (?, 1, 1)\n",
    "    V_xx = tf.expand_dims(V_xx[:, 0, 0], axis=-1)  # d²V/dx²\n",
    "\n",
    "    # Compute the PDE residual\n",
    "    diff_V = V_t+V_x*(1/2*Z*Z-1/2*beta*beta-alpha)+1/2*V_xx*Z*Z+(1-beta)*(beta+Z)-alpha\n",
    "\n",
    "    L1 = tf.reduce_mean(tf.square(diff_V))  # Loss term for PDE\n",
    "\n",
    "    target_terminal = terminal_utility(X_terminal)\n",
    "    fitted_terminal = model(t_terminal, X_terminal)\n",
    "    diff_terminal = fitted_terminal - target_terminal\n",
    "    L3 = tf.reduce_mean(tf.square(diff_terminal))  # Loss term for terminal condition\n",
    "    \n",
    "    del gt\n",
    "    return L1, L3, diff_V, diff_terminal\n",
    "\n",
    "def loss_control(model, control, t_interior, X_interior, t_terminal, X_terminal):\n",
    "    ''' Compute total loss for training.'''\n",
    "    with tf.GradientTape(persistent=True, watch_accessed_variables=False) as gt:\n",
    "        gt.watch(t_interior)\n",
    "        gt.watch(X_interior)\n",
    "        V = model(t_interior, X_interior)  # V shape: (?, 1)\n",
    "        V_x = gt.gradient(V, X_interior)   # First-order gradients, shape: (?, 1)\n",
    "\n",
    "    out = control(t_interior, X_interior)\n",
    "    Z = tf.expand_dims(out[:,0],axis=-1)  # Control function, shape: (?, 1)\n",
    "    alpha=tf.expand_dims(out[:,1],axis=-1) # Control function, shape: (?, 1)\n",
    "    beta=tf.expand_dims(out[:,2],axis=-1) # Control function, shape: (?, 1)\n",
    "    # Compute second-order derivatives\n",
    "    V_xx = gt.batch_jacobian(V_x, X_interior)  # shape (?, 1, 1)\n",
    "    V_xx = tf.expand_dims(V_xx[:, 0, 0], axis=-1)  # d²V/dx²\n",
    "\n",
    "    # Compute the PDE residual\n",
    "    f=V_x*(1/2*Z*Z-1/2*beta*beta-alpha)+1/2*V_xx*Z*Z+(1-beta)*(beta+Z)-alpha # shape (?, 1)\n",
    "    dfdalpha= -V_x -1\n",
    "\n",
    "    #penalty=tf.nn.relu(beta-Cbeta_up)+tf.nn.relu(Cbeta_low-beta)+tf.nn.relu(beta+Z-CbetaZ_up)+tf.nn.relu(CbetaZ_low-Z-beta) #case 1\n",
    "    #penalty=tf.nn.relu(beta-Cbeta_up)+tf.nn.relu(Cbeta_low-beta) #case 2\n",
    "    penalty = tf.nn.relu(beta-Cbeta_up)+tf.nn.relu(beta+Z-CbetaZ_up)#case 3\n",
    "    controlbound=tf.reduce_mean(penalty) \n",
    "    \n",
    "    L2 = tf.reduce_mean(f)-controlbound  # Loss term for control\n",
    "    dfdalpha = tf.reduce_mean(dfdalpha) # derivative of f(L2) on control\n",
    "\n",
    "    del gt\n",
    "    return -L2, dfdalpha, controlbound, penalty\n",
    "\n",
    "#%% Set up network\n",
    "\n",
    "# initialize DGM model\n",
    "model = DGMNet(nodes_per_layer, num_layers, input_dim=d, output_dim=1)\n",
    "control = DGMNet(nodes_per_layer, num_layers, input_dim=d,output_dim=3, control_output=True)\n",
    "# controlalpha=DGMNet(nodes_per_layer, num_layers, input_dim=d,output_dim=1, control_output=True)\n",
    "# controlbeta=DGMNet(nodes_per_layer, num_layers, input_dim=d,output_dim=1, control_output=True)\n",
    "\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=0.001, decay_steps=10000, end_learning_rate=0.00001, power=1.2\n",
    ")\n",
    "\n",
    "\n",
    "# Optimizers\n",
    "optimizer_value = tf.keras.optimizers.Adam(lr_schedule)\n",
    "optimizer_control = tf.keras.optimizers.Adam(lr_schedule)\n",
    "# optimizer_control_alpha = tf.keras.optimizers.Adam(lr_schedule)\n",
    "# optimizer_control_beta = tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "\n",
    "#%% Train network\n",
    "\n",
    "# Define the train_step function\n",
    "@tf.function\n",
    "def train_step(model, control,  t_interior, X_interior, t_terminal, X_terminal, optimizer_value, optimizer_control):\n",
    "    ''' Perform of single training step.'''\n",
    "    # Compute loss for the value function (L1 + L3)\n",
    "    t_interior = tf.convert_to_tensor(t_interior, dtype=tf.float32)\n",
    "    X_interior = tf.convert_to_tensor(X_interior, dtype=tf.float32)\n",
    "    t_terminal = tf.convert_to_tensor(t_terminal, dtype=tf.float32)\n",
    "    X_terminal = tf.convert_to_tensor(X_terminal, dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape1:\n",
    "        L1, L3, diff_V, diff_terminal = loss(model, control, t_interior, X_interior, t_terminal, X_terminal)\n",
    "        total_loss = L1+L3\n",
    "    grads = tape1.gradient(total_loss, model.trainable_variables)\n",
    "\n",
    "    optimizer_value.apply_gradients(zip(grads, model.trainable_variables))  # Update the value model\n",
    "    \n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        L2, _, _ ,_= loss_control(\n",
    "            model, control,\n",
    "            t_interior, X_interior,\n",
    "            t_terminal, X_terminal\n",
    "        )\n",
    "\n",
    "\n",
    "    grads_control = tape.gradient(L2, control.trainable_variables)\n",
    "\n",
    "\n",
    "    optimizer_control.apply_gradients(zip(grads_control, control.trainable_variables))\n",
    "\n",
    "    \n",
    "    return L1, L3, diff_V, diff_terminal, L2, total_loss\n",
    "@tf.function\n",
    "def eval_step(model, control, t_interior_val, X_interior_val, t_terminal_val, X_terminal_val):\n",
    "    L1_val, L3_val, diff_V_val, diff_terminal_val = loss(model, control, t_interior_val, X_interior_val, t_terminal_val, X_terminal_val)\n",
    "    L2_val, dL2dalpha_val, controlbound_val, penalty_val = loss_control(model, control, t_interior_val, X_interior_val, t_terminal_val, X_terminal_val)\n",
    "    return L1_val, L3_val, diff_V_val, diff_terminal_val, L2_val, dL2dalpha_val, controlbound_val, penalty_val\n",
    "\n",
    "# Initialize lists for storing loss values\n",
    "loss_list = []\n",
    "L2_list = []\n",
    "L1_list = []\n",
    "L3_list = []\n",
    "value_list = []\n",
    "control_list = []\n",
    "penalty_list = []\n",
    "maxdiffV_list = []\n",
    "cb_list=[]\n",
    "\n",
    "maxdiff_V = 100\n",
    "maxdiff_terminal = 100\n",
    "\n",
    "# dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "t_eval = tf.constant([[0.0]], dtype=tf.float32)  # shape (1, 1)\n",
    "X_eval = tf.constant([[0.0]], dtype=tf.float32)  # shape (1, 1)\n",
    "\n",
    "# Main training loop\n",
    "for i in range(sampling_stages):\n",
    "    t_interior, X_interior, t_terminal, X_terminal = sampler(nSim_interior, nSim_terminal)\n",
    "    for _ in range(steps_per_sample):\n",
    "        L1, L3, diff_V, diff_terminal, L2, total_loss = train_step(\n",
    "                model, control, t_interior, X_interior,\n",
    "                t_terminal, X_terminal,\n",
    "                optimizer_value, optimizer_control\n",
    "            )\n",
    "    loss_list.append(total_loss)\n",
    "    L2_list.append(L2)\n",
    "    L1_list.append(L1)\n",
    "    L3_list.append(L3)\n",
    "\n",
    "    # val\n",
    "    t_interior_val, X_interior_val, t_terminal_val, X_terminal_val = sampler(nSim_interior, nSim_terminal)\n",
    "    t_interior_val = tf.convert_to_tensor(t_interior_val, dtype=tf.float32)\n",
    "    X_interior_val = tf.convert_to_tensor(X_interior_val, dtype=tf.float32)\n",
    "    t_terminal_val = tf.convert_to_tensor(t_terminal_val, dtype=tf.float32)\n",
    "    X_terminal_val = tf.convert_to_tensor(X_terminal_val, dtype=tf.float32)\n",
    "\n",
    "    L1_val, L3_val, diff_V_val, diff_terminal_val, L2_val, dL2dalpha_val, controlbound_val, penalty_val = eval_step(\n",
    "        model, control, t_interior_val, X_interior_val, t_terminal_val, X_terminal_val\n",
    "    )\n",
    "\n",
    "    maxdiff_V = np.max(np.abs(diff_V_val.numpy()))\n",
    "    maxdiff_terminal = np.max(np.abs(diff_terminal_val.numpy()))\n",
    "    maxdL2dalpha = np.max(np.abs(dL2dalpha_val.numpy()))\n",
    "    maxControlBound= np.max(np.abs(controlbound_val.numpy()))\n",
    "    maxPenalty = np.max(np.abs(penalty_val.numpy()))\n",
    "    penalty_list.append(maxPenalty)\n",
    "    maxdiffV_list.append(maxdiff_V)\n",
    "    cb_list.append(maxControlBound)\n",
    "\n",
    "    print(f\"Stage {i}:{maxdiff_V},{maxdiff_terminal},{maxdL2dalpha},{maxControlBound},{maxPenalty}, L1 = {L1.numpy()}, L3 = {L3.numpy()}, L2 = {L2.numpy()}\")\n",
    "\n",
    "    # Evaluate at (t=0, x=0)\n",
    "    V_pred = model(t_eval, X_eval)\n",
    "    value_list.append(V_pred.numpy()[0, 0])\n",
    "    print(\"V(0, 0) =\", V_pred.numpy()[0, 0])\n",
    "    out_pred = control(t_eval, X_eval)\n",
    "    control_pred = tf.expand_dims(out_pred[:,0],axis=-1)  # Control function, shape: (?, 1)\n",
    "    control_alpha_pred = tf.expand_dims(out_pred[:,1],axis=-1) # Control function, shape: (?, 1)\n",
    "    control_beta_pred = tf.expand_dims(out_pred[:,2],axis=-1) # Control function, shape\n",
    "    control_list.append(control_pred.numpy()[0, 0])\n",
    "    print(\"z(0, 0) =\", control_pred.numpy()[0, 0])\n",
    "    print(\"alpha(0, 0) =\", control_alpha_pred.numpy()[0, 0])\n",
    "    print(\"beta(0, 0) =\", control_beta_pred.numpy()[0, 0])\n",
    "    print(\"beta+z =\",control_pred.numpy()[0, 0]+control_beta_pred.numpy()[0, 0])\n",
    "\n",
    "    if maxdiff_V < 1e-2 and maxdL2dalpha < 1e-3 and maxPenalty==0:\n",
    "        break\n",
    "\n",
    "# Save output if needed\n",
    "if saveOutput:\n",
    "    model.save(f'./SavedNets/{saveName}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75392326-599b-443b-add8-28be0dbd324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this plot beta\n",
    "# vector of t and X values for plotting\n",
    "X_plot = np.linspace(X_low, X_high, n_plot)\n",
    "t_plot = np.linspace(t_low, T, n_plot)\n",
    "    \n",
    "# compute model-implied optimal control for each (t,X) pair\n",
    "t_mesh, X_mesh = np.meshgrid(t_plot, X_plot)\n",
    "\n",
    "t_plot = np.reshape(t_mesh, [n_plot**2,1])\n",
    "X_plot = np.reshape(X_mesh, [n_plot**2,1])\n",
    "\n",
    "t_plot = tf.convert_to_tensor(t_plot, dtype=tf.float32)\n",
    "X_plot = tf.convert_to_tensor(X_plot, dtype=tf.float32)\n",
    "\n",
    "fitted_optimal_control_out = control(t_plot, X_plot)\n",
    "fitted_optimal_control_beta = tf.expand_dims(fitted_optimal_control_out[:,2],axis=-1)\n",
    "# fitted_optimal_control_beta = controlbeta(t_plot, X_plot)\n",
    "fitted_optimal_control_beta_mesh = np.reshape(fitted_optimal_control_beta, [n_plot, n_plot])\n",
    "\n",
    "print(fitted_optimal_control_beta_mesh)\n",
    "\n",
    "plt.rcParams['axes.spines.right'] = True\n",
    "plt.rcParams['axes.spines.top'] = True\n",
    "# PLOT optimal beta\n",
    "c=np.min(fitted_optimal_control_beta_mesh)\n",
    "d=np.max(fitted_optimal_control_beta_mesh)\n",
    "fig, ax = plt.subplots(figsize = (12,12),dpi=500)\n",
    "#plt.figure(figsize = (8,8),dpi=500)\n",
    "levels = np.arange(c,d,0.1)\n",
    "\n",
    "#CB=plt.pcolormesh(t_mesh, X_mesh,A, cmap = \"rainbow\",vmin=c ,vmax=c)\n",
    "\n",
    "CS=plt.contour(t_mesh, X_mesh,fitted_optimal_control_beta_mesh, levels=levels)\n",
    "plt.clabel(CS, inline=True,fontsize=20)\n",
    "# plot options\n",
    "#a=plt.colorbar(CS)\n",
    "#for t in a.ax.get_yticklabels():\n",
    "     #t.set_fontsize(10)\n",
    "#plt.title(\"numerical control for c=0\", fontsize=20)\n",
    "plt.ylabel(\"x\", fontsize=25, labelpad=10)\n",
    "plt.xlabel(\"t\", fontsize=25, labelpad=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.savefig('beta.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c79b7-6ac7-4a16-adab-86efa736589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this plot beta+z\n",
    "# vector of t and X values for plotting\n",
    "X_plot = np.linspace(X_low, X_high, n_plot)\n",
    "t_plot = np.linspace(t_low, T, n_plot)\n",
    "    \n",
    "# compute model-implied optimal control for each (t,X) pair\n",
    "t_mesh, X_mesh = np.meshgrid(t_plot, X_plot)\n",
    "\n",
    "t_plot = np.reshape(t_mesh, [n_plot**2,1])\n",
    "X_plot = np.reshape(X_mesh, [n_plot**2,1])\n",
    "\n",
    "t_plot = tf.convert_to_tensor(t_plot, dtype=tf.float32)\n",
    "X_plot = tf.convert_to_tensor(X_plot, dtype=tf.float32)\n",
    "\n",
    "\n",
    "fitted_optimal_control_out = control(t_plot, X_plot)\n",
    "fitted_optimal_control = tf.expand_dims(fitted_optimal_control_out[:,0],axis=-1)\n",
    "fitted_optimal_control_mesh = np.reshape(fitted_optimal_control, [n_plot, n_plot])\n",
    "\n",
    "print(fitted_optimal_control_mesh+fitted_optimal_control_beta_mesh)\n",
    "\n",
    "plt.rcParams['axes.spines.right'] = True\n",
    "plt.rcParams['axes.spines.top'] = True\n",
    "# PLOT optimal beta\n",
    "c=np.min(fitted_optimal_control_mesh+fitted_optimal_control_beta_mesh)\n",
    "d=np.max(fitted_optimal_control_mesh+fitted_optimal_control_beta_mesh)\n",
    "fig, ax = plt.subplots(figsize = (12,12),dpi=500)\n",
    "#plt.figure(figsize = (8,8),dpi=500)\n",
    "levels = np.arange(c,d,0.004)\n",
    "\n",
    "#CB=plt.pcolormesh(t_mesh, X_mesh,A, cmap = \"rainbow\",vmin=c ,vmax=c)\n",
    "\n",
    "CS=plt.contour(t_mesh, X_mesh,fitted_optimal_control_mesh+fitted_optimal_control_beta_mesh, levels=levels)\n",
    "plt.clabel(CS, inline=True,fontsize=20)\n",
    "# plot options\n",
    "#a=plt.colorbar(CS)\n",
    "#for t in a.ax.get_yticklabels():\n",
    "     #t.set_fontsize(10)\n",
    "#plt.title(\"numerical control for c=0\", fontsize=20)\n",
    "plt.ylabel(\"x\", fontsize=25, labelpad=10)\n",
    "plt.xlabel(\"t\", fontsize=25, labelpad=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.savefig('beta+Z.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca3b965-066b-4e31-96be-e57316d1805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lolt(loss):\n",
    "    res = []\n",
    "    for i in range (len(loss)):\n",
    "        if (loss[i]<10**-8 or i%5!=0):\n",
    "            res.append(float(\"NaN\"))\n",
    "        else:\n",
    "            res.append(loss[i])\n",
    "    return res\n",
    "\n",
    "def lola(loss):\n",
    "    res = []\n",
    "    for i in range (len(loss)):\n",
    "        if (loss[i]<10**-8 and i%5==0):\n",
    "            res.append(10**-8)\n",
    "        else:\n",
    "            res.append(float(\"NaN\"))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cb1a11-f149-4132-ad49-75cde450a90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.ticker as ticker\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "fig, ax = plt.subplots(figsize = (12,12),dpi=500)\n",
    "#fig,ax = plt.figure()\n",
    "#plt.figure(figsize = (8,8),dpi=500)\n",
    "#fig, ax = plt.subplots()\n",
    "x=np.linspace(0,10*len(L1_list),len(L1_list))\n",
    "plt.plot(x,L1_list,label='$||L_{int}||_2$',color='blue')\n",
    "plt.plot(x,maxdiffV_list,label='$||L_{int}||_{\\infty}$',color='blue',linestyle='dashed')\n",
    "plt.plot(x,lolt(cb_list),'g^',label='$||P(\\\\beta,Z)||_{1}$',color='green')\n",
    "plt.plot(x,lola(cb_list),'g^',color='green',linewidth=5)\n",
    "plt.plot(x,lolt(penalty_list),'x',color='green',label='$||P(\\\\beta,Z)||_{\\infty}$')\n",
    "plt.plot(x,lola(penalty_list),'x',color='green',linestyle='dashed')\n",
    "#plt.title(\"L2\", fontsize=20)\n",
    "plt.ylabel(\"Loss function\", fontsize=25, labelpad=20)\n",
    "plt.xlabel(\"Training step n\", fontsize=25, labelpad=20)\n",
    "plt.xticks(fontsize=20)\n",
    "#fig, ax = plt.subplots()\n",
    "plt.yticks(fontsize=20)\n",
    "plt.yscale('log')\n",
    "#plt.xlim([0, 5*len(L1_list)+10])\n",
    "plt.ylim([10**-8, 10])\n",
    "#_labels = ax.get_yticks()\n",
    "#ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%0.0e'))\n",
    "#plt.ticklabel_format(axis='y', style='scientific')\n",
    "#fig.get_axes()[0].ticklabel_format(axis='y', style='scientific')\n",
    "#ax.ticklabel_format(axis='y', style='sci',useMathText=True)\n",
    "plt.yticks(np.array([0.0000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10**1]),fontsize=20)\n",
    "ax.set_yticklabels(['$10^{-7}$','$10^{-6}$','$10^{-5}$','$10^{-4}$','$10^{-3}$','$10^{-2}$','$10^{-1}$','$10^0$','$10^1$'])\n",
    "#locmaj = ticker.LogLocator(base=10,numticks=12) \n",
    "#ax.yaxis.set_major_locator(locmaj)\n",
    "#plt.gca().set_yticklabels()\n",
    "plt.legend(fontsize=\"25\", loc =\"upper right\",framealpha=0.2)\n",
    "#plt.box(False)\n",
    "plt.savefig('loss function.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647b8b5a-2b0e-4cf6-ad5d-c8c87846f954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models weights .h5\n",
    "model.save_weights('model_value.weights.h5')\n",
    "control.save_weights('model_control_z.weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6e4c95-32f4-4382-a125-ec243c5e987f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782caa11-af5d-4da3-bc2c-ef13f873ccee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d346a6-47c5-47f5-8ff4-32682a045169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877cceb4-bd8d-415d-8ed5-780be13d0bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e821eedc-c69e-40ea-bb04-123c0352c73c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1b31fa-6ac8-4660-add9-093109d64bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372729e8-d024-4d9a-8549-580896357878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19226f17-5aba-409b-923e-d6bee24683c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed8d96f-61c8-4b47-aa42-3e226fae7e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da067ca3-9021-40b9-bc2d-320c570d2130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee2a58b-c140-41e3-98b2-73e72988a8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 24239.341074,
   "end_time": "2025-06-13T00:45:24.526530",
   "environment_variables": {},
   "exception": null,
   "input_path": "x5.ipynb",
   "output_path": "x51.ipynb",
   "parameters": {},
   "start_time": "2025-06-12T18:01:25.185456",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
